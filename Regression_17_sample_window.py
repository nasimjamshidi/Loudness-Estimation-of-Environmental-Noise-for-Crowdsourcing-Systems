# -*- coding: utf-8 -*-
"""HW3_Regression_17-sample-window.ipynb.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-Y6icLXJoAT69OJ_fYME_EwuGRN-qXKC
"""



from sklearn.datasets import make_classification
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from skorch import NeuralNetClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn import preprocessing
from sklearn.model_selection import cross_val_predict
from sklearn.model_selection import GroupShuffleSplit
from skorch import NeuralNetRegressor
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from scipy.stats import pearsonr
import seaborn as sns; sns.set()
import matplotlib as plt
import statistics 
import numpy as np
import re
import os
import pandas as pd
import librosa
import torch
from torch import nn
import pickle
from math import sqrt



# ==========================Define the required variables======================
Mean=[]
STD=[]
labels=[]
file_names_class=[]
file_names_regr=[]
All_mfccs=[]
noise_Values=[]
accuracy_classification=[]
accuracy_classification_Mean=[]
accuracy_classification_total=[]
accuracy_regression=[]
accuracy_score_r2=[]
accuracy_regression_Mean=[]
accuracy_regression_total=[]
accuracy_rmse=[]
# ========================Store all .webm files in a list======================
path_to_webm=r'D:\Sound'
webm_files = [pos_webm for pos_webm in os.listdir(path_to_webm) if pos_webm.endswith('.webm')] 

# ==============================Load the files=================================
n_mfcc=30
for k in webm_files:
        ff,sr = librosa.load(path_to_webm +  '//' + k) #load the files via librosa
        mfcc = librosa.feature.mfcc(y=ff, sr=sr, n_mfcc=n_mfcc) #Extract mfcc coefficients 
        
      # Append the Mean values of all coefficients into a list as Mean
        
     
        file_names_class.append('_'.join(k.split('_')[:1]))# Split the file names and get the class names for classification
                                                           #(e.g.'autos_JamshidiAvanaki_48dBA_1590939394506.webm'-->'autos')
                                                           
        file_names_regr.append('_'.join(k.split('_')[2:3]))# Split the file names and get the noise level for regression
                                                           #(e.g.'autos_JamshidiAvanaki_48dBA_1590939394506.webm'-->'48')
        mfcc_T=np.transpose(mfcc)

        noise_Value =int(re.search(r'\d+', k).group())# Store all noise values as int at noise_Values list
  # =============================================================================
        for i in range(0,np.shape(mfcc_T)[0],17):
              #mfcc_Mean=mfcc_T[i]

              mfcc_Mean=np.mean(mfcc_T[i:i+17] , axis=0)
              #mfcc_STD=np.std(mfcc_T[i:i+17] , axis=0)
              Mean.append(mfcc_Mean)
              #STD.append(mfcc_STD)
              labels.append(file_names_class[-1])
              noise_Values.append(noise_Value)
              #labels.append(file_names_class[-1])
# =============================================================================

     
df_class_names = pd.DataFrame(labels)# Convert list to DataFrame
df_noise_Values = pd.DataFrame(noise_Values)# Convert list to DataFrame
df_mfcc_Mean = pd.DataFrame(Mean)# Convert list to DataFrame
#df_mfcc_Mean = pd.DataFrame(df_Mean)# Convert list to DataFrame

# ============================Encode the class labels===========================  
labelencoder = LabelEncoder()# Create instance of labelencoder
df_class_names = labelencoder.fit_transform(df_class_names) # Assign numerical values for df_class_names

# ========Define the Features and Labels for Classification and Regression=====

# Define X as features and y as lable for Classification part
X=(df_mfcc_Mean).to_numpy()
y=df_class_names
X = X.astype(np.float32) # Change the type of X to float32
y = y.astype(np.int64) # Change the type of y to int64 
  
# Define X_regr as features and y_regr as lable for Regression part
X_regr = X
y_regr=(df_noise_Values).to_numpy()
y_regr = y_regr.astype(np.float32)/100
y_regr = y_regr.reshape(-1, 1)   

# ===============Preprocessing the features using StandardScaler===============
scaler = preprocessing.StandardScaler()
X = scaler.fit_transform(X) # Transform the feature
X_regr = scaler.fit_transform(X_regr)



# =======================Neural Network for Regression=========================
class Net(torch.nn.Module):
    def __init__(self):
        '''
        A feedForward neural network.
        Argurmets:
            n_feature: How many of features in your data
            n_hidden:  How many of neurons in the hidden layer
            n_output:  How many of neuros in the output leyar (defaut=1)
        '''
        super(Net, self).__init__()
        self.hidden1 = torch.nn.Linear(n_mfcc, 100, bias=True)   # hidden layer
        self.predict = torch.nn.Linear(100, 1, bias=True)   # output layer
    def forward(self, x,**kwargs):
        '''
        Argurmets:
            x: Features to predict
        '''
        torch.nn.init.constant_(self.hidden1.bias.data,1)
        torch.nn.init.constant_(self.predict.bias.data,1)
        x = torch.sigmoid(self.hidden1(x))      # activation function for hidden layer
        x = torch.sigmoid(self.predict(x))     # linear output
        return x
    
# =========================== Define the Regression network====================
net_regr = NeuralNetRegressor(
    Net,
    max_epochs=10,
    lr=0.1,
    verbose=1,
)

# =================Split the dataset using GroupShuffleSplit=================== 
gss= GroupShuffleSplit(n_splits=10,test_size=0.15,random_state=42) 
gss.get_n_splits(X_regr,y_regr,groups=y)

for train_index, test_index in (gss.split(X_regr,y_regr, groups=y)):
   print("TRAIN:", train_index, "TEST:", test_index)
   X_regr_train,X_regr_test = X_regr[train_index],X_regr[test_index]
   y_regr_train,y_regr_test = y_regr[train_index],y_regr[test_index]

# ==============================Train the model================================
   net_regr.fit(X_regr_train, y_regr_train)
   
# ================Test/Validate the model via 10fold crossValidation===========
   y_regr_pred=cross_val_predict(net_regr, X_regr_test, y_regr_test, cv=10)
  
# ============================ Evaluate the model =============================
   score_regression=r2_score(y_regr_test,y_regr_pred)
   score_r2=pearsonr(y_regr_test.flatten(),y_regr_pred.flatten())[0]**2
   rmse=sqrt(mean_squared_error(y_regr_test, y_regr_pred))
   #score_regression=mean_squared_error(y_regr_pred, y_regr_test)
   accuracy_regression.append(score_regression)
   accuracy_score_r2.append(score_r2)
   accuracy_rmse.append(rmse)

print(accuracy_regression)
print(score_r2)

#accuracy_regression_Mean = statistics.mean(map(float, accuracy_regression))
#accuracy_regression_Mean=round(accuracy_regression_Mean, 5)
#accuracy_regression_total.append(accuracy_regre ssion_Mean)
    
# ======================= Save the model with pickle ==========================    
# =============================================================================
# filename = 'Regression_model.sav'
# pickle.dump(net_regr, open(filename, 'wb'))   
# =============================================================================

# ======================= Save the model with pickle ==========================    

filename = 'Regression_model_HW3.sav'
pickle.dump(net_regr, open(filename, 'wb'))
accuracy_regression=pd.DataFrame(accuracy_regression)
accuracy_regression.to_csv(r'C:\Users\Nasim\Desktop\accuracy_regression15')

accuracy_score_r2=pd.DataFrame(accuracy_score_r2)
accuracy_score_r2.to_csv(r'C:\Users\Nasim\Desktop\accuracy_score_r215')

accuracy_rmse=pd.DataFrame(accuracy_rmse)
accuracy_rmse.to_csv(r'C:\Users\Nasim\Desktop\accuracy_rmse15')